{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRMAS_Deep_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-hm/IRMAS_DeepLearning/blob/master/IRMAS_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByekeUvSYJX9",
        "colab_type": "text"
      },
      "source": [
        "# Final project : Instrument recognition in musical audio signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ErozRfdg9V",
        "colab_type": "text"
      },
      "source": [
        "# 0. Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExLw7mtFgoYJ",
        "colab_type": "text"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgC2UBSHdlJ9",
        "colab_type": "code",
        "outputId": "c8cb269d-7389-4941-cb2f-d7401415d9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "##Opening Data And saving\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls '/content/drive/My Drive/DeepLearning_2020/Final Project/Data/'\n",
        "\n",
        "data_path = '/content/drive/My Drive/DeepLearning_2020/Final Project/Data/'\n",
        "results_path = '/content/drive/My Drive/DeepLearning_2020/Final Project/Results/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cel\t\t\t  flu\t\t labels.npy  sax\t   vio\n",
            "cla\t\t\t  gac\t\t org\t     specs_11.npy  voi\n",
            "data_11_instruments.json  gel\t\t pia\t     specs.npy\n",
            "data_jason.json\t\t  labels_11.npy  README.txt  tru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKHHHT_gr0m",
        "colab_type": "text"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ4Pq6rTgkyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install essentia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDa6BwZrgvah",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS-0TjqVcGKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9c65950-0e72-4f82-fe38-2aae11b6647e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "import essentia.standard as es\n",
        "from essentia import Pool\n",
        "import sklearn\n",
        "\n",
        "import random\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BUn1NwwrlOW",
        "colab_type": "text"
      },
      "source": [
        "### Warm up CPU and GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS1nt8F8sH7_",
        "colab_type": "code",
        "outputId": "fcc5f99f-67f2-4282-9ec9-b3062aea591c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-fZz45fYqXH",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsB2WCqDDvo3",
        "colab_type": "text"
      },
      "source": [
        "###Store filenames in dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmC3bn_QC6r0",
        "colab_type": "text"
      },
      "source": [
        "This dictionary maps folder names (alias for musical instruments) to numerical indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc66zdHDkh_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASS_MAPPING = {\n",
        "    'flu': 0, \n",
        "    'tru': 1, \n",
        "    'voi': 2, \n",
        "    'gac': 3, \n",
        "    'pia': 4\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQOm4KS-DcN4",
        "colab_type": "text"
      },
      "source": [
        "First, we store the filenames in a dictionary, where the keys are the folder names (as in CLASS_MAPING)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Deu8vhNgxpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_files = dict()\n",
        "\n",
        "for i in CLASS_MAPPING.keys():\n",
        "  files = []\n",
        "  for filename in os.listdir(data_path + \"/\" + i + \"/\"):      \n",
        "    files.append(filename)\n",
        "  audio_files[i] = files\n",
        "print(audio_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k0X-v_KkiFk",
        "colab_type": "text"
      },
      "source": [
        "###Save json file in the Data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUGYfs4SD13G",
        "colab_type": "text"
      },
      "source": [
        "Then, we save the previously created dictionary in a json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMMZMBN3jmfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_path +'/data_jason.json', 'w') as outfile:\n",
        "    json.dump(audio_files, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EttNzu-tGSR3",
        "colab_type": "text"
      },
      "source": [
        "### Functions to compute spectogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I7Hkaz4ELlS",
        "colab_type": "text"
      },
      "source": [
        "To classify samples we will compute their Log Mel Spectrograms using Essentia library. These are Spectrogram with the Log Mel Scale as its y axis.  This Mel Scale is constructed such that sounds of equal distance from each other on the Mel Scale, also “sound” to humans as they are equal in distance from one another. (https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkRBs0ZfhyiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad(l, sr):\n",
        "    # 0-Pad 10 sec at fs hz and add little noise\n",
        "    z = np.zeros(10*sr, dtype='float32')\n",
        "    z[:l.size] = l\n",
        "    z = z + 5*1e-4*np.random.rand(z.size).astype('float32')\n",
        "    return z\n",
        "\n",
        "\n",
        "def compute_spectrogram(filename, sr=16000, n_mels=64):\n",
        "    # compute log mel magnitude spectrogram \n",
        "    audio = es.MonoLoader(filename=filename,sampleRate=sr)()\n",
        "    audio = pad(audio, sr)\n",
        "    \n",
        "    # essentia extractor\n",
        "    windowing = es.Windowing(type='hann', normalized=False, zeroPadding=0)\n",
        "    spectrum = es.Spectrum()\n",
        "    melbands = es.MelBands(numberBands=n_mels, \n",
        "                           sampleRate=sr, \n",
        "                           lowFrequencyBound=0, \n",
        "                           highFrequencyBound=8000, \n",
        "                           inputSize=(2048)//2+1, \n",
        "                           weighting='linear', \n",
        "                           normalize='unit_tri', \n",
        "                           warpingFormula='slaneyMel', \n",
        "                           type='power')\n",
        "    \n",
        "    norm10k = es.UnaryOperator(type='identity', shift=0, scale=1)\n",
        "    log10 = es.UnaryOperator(type='log10')\n",
        "    results = Pool()\n",
        "\n",
        "    for frame in es.FrameGenerator(audio, frameSize=1024, hopSize=512, startFromZero=False):\n",
        "        spectrumFrame = spectrum(windowing(frame))     \n",
        "        results.add('melbands', log10(norm10k(melbands(spectrumFrame))))\n",
        "    \n",
        "    mel_log_spectrum = results['melbands']\n",
        "    return mel_log_spectrum[:64].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gHmW3yGbMi",
        "colab_type": "text"
      },
      "source": [
        "### Example of computing spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYI-FhsfHy0F",
        "colab_type": "text"
      },
      "source": [
        "Here, we show an example of computing a spectrogram from a random file in the dataset. Also, the random track can be reproduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp25KOCsYwri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select random file\n",
        "random_instrument = random.choice(list(CLASS_MAPPING.keys()))\n",
        "rand_sound = random.choice(audio_files[random_instrument])\n",
        "data_rand_instrument = data_path + '/' + random_instrument + '/'\n",
        "\n",
        "\n",
        "#compute spectrogram and reproduce the track\n",
        "x = compute_spectrogram(f'{data_rand_instrument}/{rand_sound}')\n",
        "librosa.display.specshow(x, sr=16000, x_axis='time', y_axis='mel')\n",
        "\n",
        "print(rand_sound)\n",
        "Audio(f'{data_rand_instrument}/{rand_sound}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJrdTupHGgME",
        "colab_type": "text"
      },
      "source": [
        "### Compute spectrogram for whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So8g_OEdIAtK",
        "colab_type": "text"
      },
      "source": [
        "If this cell is activated to true, it will recompute all the spectrograms for the files in the dataset. It takes approximately 30 minutes to complete. For this reason, we have stored them in the file specs.npy and the labels in labels.npy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ogA3AQmvrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "    dataset = json.load(open(data_path+'/data_jason.json', 'rb'))\n",
        "    num_sounds = sum([len(v) for _, v in dataset.items()])\n",
        "    num_classes = len(dataset)\n",
        "\n",
        "    specs = np.zeros((num_sounds, 64, 64))\n",
        "    labels = np.zeros((num_sounds,))\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    with tqdm(total=num_sounds) as pbar:\n",
        "        for class_name, sounds in dataset.items():\n",
        "            for sound in sounds:\n",
        "                specs[idx] = compute_spectrogram(f'{data_path}{class_name}/{sound}')\n",
        "                # labels[idx][NSYNTH_CLASS_MAPPING[class_name]] = 1\n",
        "                labels[idx] = CLASS_MAPPING[class_name]\n",
        "                idx += 1\n",
        "                pbar.update()\n",
        "\n",
        "    np.save(data_path+'specs.npy', specs)\n",
        "    np.save(data_path+'labels.npy', labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlp5qfDiY2Xb",
        "colab_type": "text"
      },
      "source": [
        "# 2. Create custom datasaet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tHsMIpVI3iz",
        "colab_type": "text"
      },
      "source": [
        "First, we load the numpy files that contain the spectrograms and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7_M797eY-Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "specs = np.load(data_path+'specs.npy')\n",
        "labels = np.load(data_path+'labels.npy')\n",
        "\n",
        "#reshape the spectrograms\n",
        "specs = specs.reshape(-1, 64, 64, 1)\n",
        "#specs = (specs - np.mean(specs)) / np.std(specs)\n",
        "\n",
        "# split train test\n",
        "X_train, X_test, y_train, y_test = train_test_split(specs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"There are \" + str(len(X_train)) + \" training samples and \" + str(len(X_test))+ \" testing samples.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRCB0hyrJaj1",
        "colab_type": "text"
      },
      "source": [
        "Then, following the procedure presented in S2, we have created a Dataset class for the IRMAS data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh6lM4UYZQjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making native class loader\n",
        "class IRMAS(torch.utils.data.Dataset):\n",
        "    # Initialization method for the dataset\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.data = torch.from_numpy(X_train.astype(float))\n",
        "        self.labels = torch.from_numpy(y_train.astype(float)).squeeze()\n",
        "\n",
        "    \n",
        "    # What to do to load a single item in the dataset (read image and label)    \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        lbl = self.labels[index]\n",
        "\n",
        "        data = np.transpose(data, axes=[2,0,1])\n",
        "\n",
        "        return data,lbl\n",
        "    \n",
        "    \n",
        "        pass\n",
        "    \n",
        "    # Return the number of images\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujX-9aTPJ4qq",
        "colab_type": "text"
      },
      "source": [
        "We instantiate the dataset for training and the train_loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgUd11MoQHAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IRMASTrain = IRMAS(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=IRMASTrain,\n",
        "                                               batch_size=32,  \n",
        "                                               shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zRvqF76J9L_",
        "colab_type": "text"
      },
      "source": [
        "Here we show an example of usage of the data loader, extracting spectrograms from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X11lZeUTp2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for images, labels in train_loader:\n",
        "  print(images.shape, labels.shape) #images, labels shape\n",
        "  for img,lbl in zip(images,labels): \n",
        "    print('lbl : ',lbl)\n",
        "    librosa.display.specshow(img.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
        "    plt.show()\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMQ6RSdoYxJH",
        "colab_type": "text"
      },
      "source": [
        "# 3. Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm3wzZuGKj2o",
        "colab_type": "text"
      },
      "source": [
        "Our model is based in the VGG network. The main characteristics are: 3x3 convolutional filters, fully connected layers at the end, and ReLU activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH5dxH8fY059",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        \n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch3 = nn.BatchNorm2d(64)\n",
        "\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch5 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch6 = nn.BatchNorm1d(2*2*128)\n",
        "        self.fc1 = nn.Linear(2*2*128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        \n",
        "        self.maxpool= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "        self.dropout2d = nn.Dropout2d(0.05)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.batch1(out)\n",
        "        out = self.maxpool(out) # 32x32x16 --> 1x16x32x32\n",
        "\n",
        "        out = self.relu(self.conv2(out))\n",
        "        out = self.batch2(out)\n",
        "        out = self.maxpool(out) # 16x16x32\n",
        "\n",
        "        out = self.relu(self.conv3(out))\n",
        "        out = self.batch3(out)\n",
        "        out = self.maxpool(out) # 8x8x64\n",
        "\n",
        "        out = self.relu(self.conv4(out))\n",
        "        out = self.batch4(out)\n",
        "        out = self.maxpool(out) # 4x4x128\n",
        "\n",
        "        out = self.relu(self.conv5(out))\n",
        "        out = self.batch5(out)\n",
        "        out = self.maxpool(out) # 2x2x128\n",
        "\n",
        "        out = out.view(out.size(0), -1) # 512\n",
        "\n",
        "        out = self.dropout(out) # 512\n",
        "\n",
        "        out = self.batch6(out) # 512\n",
        "        \n",
        "        out = self.relu(self.fc1(out)) # 64\n",
        "\n",
        "        out = self.softmax(self.fc2(out)) # num_classes\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BT66ggWLzjK",
        "colab_type": "text"
      },
      "source": [
        "Here we instantiate our model and print the information related to each of the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIzOSU3zZfxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg = VGG()\n",
        "vgg = vgg.cuda()\n",
        "print(vgg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWRu0Vk4Zwsy",
        "colab_type": "text"
      },
      "source": [
        "# 4. Creating loss function, optimizers and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6wnKl9VL7xt",
        "colab_type": "text"
      },
      "source": [
        "For this task we use Cross Entropy Loss and Adam optimizer. The learning rate is set to 0.001 and the number of epochs to 30."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgeJv3lJZ17P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross entropy loss for classification problems\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Initialize optimizer \n",
        "learning_rate = .001\n",
        "optimizer = torch.optim.Adam(vgg.parameters(),lr = learning_rate)\n",
        "\n",
        "# Device configuration (choose GPU if it is available )\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1bT0ZMVaNJI",
        "colab_type": "text"
      },
      "source": [
        "# 5. Actual training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM2qXNTiMWoP",
        "colab_type": "text"
      },
      "source": [
        "In this section, we train our model using the described hyperparameters and the IRMAS dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDtj3koAaUPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "if True :\n",
        "    IRMASTrain = IRMAS(X_train, y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=IRMASTrain,\n",
        "                                               batch_size=64,    \n",
        "                                               shuffle=True)\n",
        "    vgg.train() # Set the model in train mode\n",
        "    total_step = len(train_loader)\n",
        "    \n",
        "    loss_list = []\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # Iterate the dataset\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Get batch of samples and labels\n",
        "            images = images.to(device).float()\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = vgg(images)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            total_loss+=loss\n",
        "                        \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if (i+1) % 20 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "        \n",
        "        avg_loss = total_loss/total_step\n",
        "        loss_list.append(avg_loss)\n",
        "    \n",
        "    # Save the model checkpoint\n",
        "    torch.save(vgg.state_dict(), results_path+'/vgg_model.ckpt')\n",
        "    #to load : model.load_state_dict(torch.load(save_name_ori))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a_5Y9uYUW74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Loss vs iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGivgxTzbOjI",
        "colab_type": "text"
      },
      "source": [
        "# 6. Analysing the output incluiding testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L5h5hRrNnT9",
        "colab_type": "text"
      },
      "source": [
        "Test the model with the test data and compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQugOefAbN7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the model\n",
        "vgg.load_state_dict(torch.load(results_path+'/vgg_model.ckpt'))\n",
        "\n",
        "# Test the model\n",
        "if True : \n",
        "    # Load test dataset\n",
        "    IRMASTest = IRMAS(X_test, y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=IRMASTest,\n",
        "                                               batch_size=16, \n",
        "                                               shuffle=True)\n",
        "    vgg.eval() # Set the model in evaluation mode\n",
        "    \n",
        "    # Compute testing accuracy\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device).float()\n",
        "            labels = labels.to(device).float()\n",
        "            # get network predictions\n",
        "            outputs = vgg(images)\n",
        "\n",
        "            # get predicted class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # compare with the ground-truth\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "        print('Test Accuracy of the model on the 633 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEXeo8ieNuXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}